services:
  comfyui-nvidia:
    # Use your preferred image tag
    # latest points to ubuntu24_cuda12.6.3-latest
    # other tags are available, such as ubuntu24_cuda13.0-latest
    # See README.md for more information
    image: mmartial/comfyui-nvidia-docker:latest
    container_name: comfyui-nvidia
    ports:
      - 8188:8188
    volumes:
      - ./run:/comfy/mnt
      - ./basedir:/basedir
#      - /usr/lib/wsl:/usr/lib/wsl # Required for Windows Subsystem for Linux 2
    restart: unless-stopped
#    extra_hosts:
#      - ollama.internal:10.12.13.14 # method to add extra hosts to the container for external services (useful for Ollama's node: use http://ollama.internal:11434)
    environment:
      # Use "uv pip" instead of "pip" (Default: false)
      - USE_UV=true
      # set WANTED_UID and WANTED_GID to your user and group as obtained with `id -u` and `id -g`
      - WANTED_UID=1000
      - WANTED_GID=1000
      # enable BASE_DIRECTORY and sets it to use the mount point to your basedir
      - BASE_DIRECTORY=/basedir
      # set SECURITY_LEVEL to normal (Default: normal)
      - SECURITY_LEVEL=normal
      # NVIDIA GPU recommended parameters
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      # Arguments to pass to ComfyUI
#      - COMFY_CMDLINE_EXTRA=--disable-api-nodes --fast fp16_accumulation --normalvram --reserve-vram 1 --fp16-vae --cuda-malloc 
# Have ComfyUI listen on 127.0.0.1:8181 and use socat to expose the service on 0.0.0.0:8188 (Default: false)
#      - USE_SOCAT=false 
# Force change directory ownership as the comfy user during script startup (Default: false)
#      - FORCE_CHOWN=false 
# Upgrade ComfyUI and other Python packages to the latest version during startup (Default: true)
#      - USE_PIPUPGRADE=true 
# Disable upgrades when starting the container (also disables USE_PIPUPGRADE) (Default: false)
#      - DISABLE_UPGRADES=false 
# Automatically install/upgrade torch. (Default: true)
#      - PREINSTALL_TORCH=true 
# Required for Windows Subsystem for Linux 2
#      - LD_LIBRARY_PATH=/usr/lib/wsl/lib 
# Add extra environment variables if needed
#      - HF_TOKEN=hf_... # set your huggingface token to avoid rate limiting
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                - gpu
                - compute
                - utility